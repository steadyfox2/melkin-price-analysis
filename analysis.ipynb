{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "001c8002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.36.0-py3-none-any.whl (9.6 MB)\n",
      "     ---------------------------------------- 9.6/9.6 MB 10.4 MB/s eta 0:00:00\n",
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (1.4.4)\n",
      "Collecting certifi>=2025.6.15\n",
      "  Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
      "     -------------------------------------- 152.9/152.9 kB 8.9 MB/s eta 0:00:00\n",
      "Collecting urllib3[socks]<3.0,>=2.5.0\n",
      "  Downloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
      "     -------------------------------------- 131.6/131.6 kB 8.1 MB/s eta 0:00:00\n",
      "Collecting trio-websocket<1.0,>=0.12.2\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Collecting trio<1.0,>=0.30.0\n",
      "  Downloading trio-0.31.0-py3-none-any.whl (512 kB)\n",
      "     ------------------------------------- 512.7/512.7 kB 10.7 MB/s eta 0:00:00\n",
      "Collecting websocket-client<2.0,>=1.8.0\n",
      "  Downloading websocket_client-1.9.0-py3-none-any.whl (82 kB)\n",
      "     ---------------------------------------- 82.6/82.6 kB 4.5 MB/s eta 0:00:00\n",
      "Collecting typing_extensions<5.0,>=4.14.0\n",
      "  Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "     ---------------------------------------- 44.6/44.6 kB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from webdriver-manager) (21.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from webdriver-manager) (2.28.1)\n",
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (1.15.1)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting exceptiongroup\n",
      "  Downloading exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from trio<1.0,>=0.30.0->selenium) (3.3)\n",
      "Collecting sniffio>=1.3.0\n",
      "  Downloading sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Collecting attrs>=23.2.0\n",
      "  Downloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
      "     ---------------------------------------- 67.6/67.6 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting wsproto>=0.14\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from urllib3[socks]<3.0,>=2.5.0->selenium) (1.7.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from packaging->webdriver-manager) (3.0.9)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from requests->webdriver-manager) (2.0.4)\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 64.7/64.7 kB ? eta 0:00:00\n",
      "Requirement already satisfied: pycparser in c:\\users\\sontaesan\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio<1.0,>=0.30.0->selenium) (2.21)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Installing collected packages: websocket-client, urllib3, typing_extensions, sniffio, python-dotenv, h11, certifi, attrs, wsproto, requests, outcome, exceptiongroup, webdriver-manager, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 0.58.0\n",
      "    Uninstalling websocket-client-0.58.0:\n",
      "      Successfully uninstalled websocket-client-0.58.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.11\n",
      "    Uninstalling urllib3-1.26.11:\n",
      "      Successfully uninstalled urllib3-1.26.11\n",
      "  Attempting uninstall: typing_extensions\n",
      "    Found existing installation: typing_extensions 4.3.0\n",
      "    Uninstalling typing_extensions-4.3.0:\n",
      "      Successfully uninstalled typing_extensions-4.3.0\n",
      "  Attempting uninstall: sniffio\n",
      "    Found existing installation: sniffio 1.2.0\n",
      "    Uninstalling sniffio-1.2.0:\n",
      "      Successfully uninstalled sniffio-1.2.0\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.5.7\n",
      "    Uninstalling certifi-2023.5.7:\n",
      "      Successfully uninstalled certifi-2023.5.7\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.4.0\n",
      "    Uninstalling attrs-21.4.0:\n",
      "      Successfully uninstalled attrs-21.4.0\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.28.1\n",
      "    Uninstalling requests-2.28.1:\n",
      "      Successfully uninstalled requests-2.28.1\n",
      "Successfully installed attrs-25.4.0 certifi-2026.1.4 exceptiongroup-1.3.1 h11-0.16.0 outcome-1.3.0.post0 python-dotenv-1.2.1 requests-2.32.5 selenium-4.36.0 sniffio-1.3.1 trio-0.31.0 trio-websocket-0.12.2 typing_extensions-4.15.0 urllib3-2.6.3 webdriver-manager-4.0.2 websocket-client-1.9.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torchvision 0.14.1 requires torch==1.13.1, but you have torch 2.0.1 which is incompatible.\n",
      "tensorflow-intel 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n",
      "pycaret 3.0.1 requires imbalanced-learn>=0.8.1, but you have imbalanced-learn 0.7.0 which is incompatible.\n",
      "pycaret 3.0.1 requires numpy<1.24,>=1.21, but you have numpy 1.24.3 which is incompatible.\n",
      "google-auth 2.19.1 requires urllib3<2.0, but you have urllib3 2.6.3 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires nbformat==5.4.0, but you have nbformat 5.5.0 which is incompatible.\n",
      "conda-repo-cli 1.0.20 requires requests==2.28.1, but you have requests 2.32.5 which is incompatible.\n",
      "botocore 1.27.28 requires urllib3<1.27,>=1.25.4, but you have urllib3 2.6.3 which is incompatible.\n",
      "autogluon-timeseries 0.8.0 requires pytorch-lightning<1.10.0,>=1.7.4, but you have pytorch-lightning 2.0.4 which is incompatible.\n",
      "autogluon-timeseries 0.8.0 requires torch<1.14,>=1.9, but you have torch 2.0.1 which is incompatible.\n",
      "autogluon-multimodal 0.8.0 requires pytorch-lightning<1.10.0,>=1.9.0, but you have pytorch-lightning 2.0.4 which is incompatible.\n",
      "autogluon-multimodal 0.8.0 requires torch<1.14,>=1.9, but you have torch 2.0.1 which is incompatible.\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -rotobuf (c:\\users\\sontaesan\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26eaf126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    product_id name price rating review_count  \\\n",
      "0  12649505033       None   None         None   \n",
      "1  12678957567       None   None         None   \n",
      "2  11391657511       None   None         None   \n",
      "3  10108843856       None   None         None   \n",
      "4  11619404547       None   None         None   \n",
      "5   9096073807       None   None         None   \n",
      "6   4758250954       None   None         None   \n",
      "7   7124664880       None   None         None   \n",
      "8  10149813702       None   None         None   \n",
      "9   6844556354       None   None         None   \n",
      "\n",
      "                                                 url  \n",
      "0  https://brand.naver.com/melkin/products/126495...  \n",
      "1  https://brand.naver.com/melkin/products/126789...  \n",
      "2  https://brand.naver.com/melkin/products/113916...  \n",
      "3  https://brand.naver.com/melkin/products/101088...  \n",
      "4  https://brand.naver.com/melkin/products/116194...  \n",
      "5  https://brand.naver.com/melkin/products/909607...  \n",
      "6  https://brand.naver.com/melkin/products/475825...  \n",
      "7  https://brand.naver.com/melkin/products/712466...  \n",
      "8  https://brand.naver.com/melkin/products/101498...  \n",
      "9  https://brand.naver.com/melkin/products/684455...  \n",
      "rows: 72\n",
      "Saved: melkin_products_step1_urls.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "BRAND_URL = \"https://brand.naver.com/melkin\"\n",
    "\n",
    "def clean_int(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    nums = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(nums) if nums else 0\n",
    "\n",
    "def clean_float(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    m = re.search(r\"(\\d+(\\.\\d+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else 0.0\n",
    "\n",
    "def extract_product_id(url: str) -> str:\n",
    "    # 보통 /products/1234567890 형태\n",
    "    if not url:\n",
    "        return \"\"\n",
    "    m = re.search(r\"/products/(\\d+)\", url)\n",
    "    return m.group(1) if m else \"\"\n",
    "\n",
    "def get_driver(headless: bool = False):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "\n",
    "    # 네이버에서 자동화 탐지 완화용 옵션들(완전 해결은 아님)\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1280,2000\")\n",
    "    chrome_options.add_argument(\"--lang=ko-KR\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.implicitly_wait(7)\n",
    "    return driver\n",
    "\n",
    "def scroll_to_load_all(driver, max_scrolls=50, pause=1.2):\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    for _ in range(max_scrolls):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(pause)\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "def scrape_products_from_brandstore(headless=False, max_scrolls=60):\n",
    "    driver = get_driver(headless=headless)\n",
    "    try:\n",
    "        driver.get(BRAND_URL)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 브랜드스토어에서 \"전체상품\" / \"스토어\" 탭 구성이 바뀔 수 있어\n",
    "        # 가능한 경우 '전체상품' 링크를 찾아 클릭\n",
    "        try:\n",
    "            all_products = driver.find_elements(By.XPATH, \"//a[contains(., '전체상품')]\")\n",
    "            if all_products:\n",
    "                all_products[0].click()\n",
    "                time.sleep(2)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # 스크롤로 상품 더 로드\n",
    "        scroll_to_load_all(driver, max_scrolls=max_scrolls, pause=1.2)\n",
    "\n",
    "        # 상품 카드(클래스명은 종종 바뀜) → 가장 안전한 건 \"products/숫자\" 링크를 모두 긁는 방식\n",
    "        anchors = driver.find_elements(By.XPATH, \"//a[contains(@href, '/products/')]\")\n",
    "\n",
    "        # 같은 상품 링크 중복 제거\n",
    "        urls = []\n",
    "        for a in anchors:\n",
    "            href = a.get_attribute(\"href\")\n",
    "            if href and \"/products/\" in href:\n",
    "                urls.append(href.split(\"?\")[0])\n",
    "        urls = list(dict.fromkeys(urls))  # preserve order, unique\n",
    "\n",
    "        data = []\n",
    "        for url in urls:\n",
    "            # 카드에서 바로 name/price/rating/review 추출하는 게 제일 좋지만\n",
    "            # 네이버는 카드 DOM이 자주 바뀌니, 1단계에서는 \"상품 URL 목록+ID\"를 확보하고\n",
    "            # 2단계에서 상세페이지에서 정밀 추출하는 2-step이 더 안정적임.\n",
    "            data.append({\n",
    "                \"product_id\": extract_product_id(url),\n",
    "                \"name\": \"\",          # 2단계에서 채움(상세페이지)\n",
    "                \"price\": None,       # 2단계에서 채움\n",
    "                \"rating\": None,      # 2단계에서 채움\n",
    "                \"review_count\": None,# 2단계에서 채움\n",
    "                \"url\": url\n",
    "            })\n",
    "\n",
    "        df = pd.DataFrame(data).drop_duplicates(subset=[\"product_id\", \"url\"])\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = scrape_products_from_brandstore(headless=False, max_scrolls=70)\n",
    "    print(df.head(10))\n",
    "    print(\"rows:\", len(df))\n",
    "\n",
    "    df.to_csv(\"melkin_products_step1_urls.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved: melkin_products_step1_urls.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5047c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/72] OK 12649505033 |  | 14700\n",
      "[2/72] OK 12678957567 |  | 7900\n",
      "[3/72] OK 11391657511 |  | 12900\n",
      "[4/72] OK 10108843856 |  | 9700\n",
      "[5/72] OK 11619404547 |  | 30000\n",
      "[6/72] OK 9096073807 |  | 5390\n",
      "[7/72] OK 4758250954 |  | 4680\n",
      "[8/72] OK 7124664880 |  | 398\n",
      "[9/72] OK 10149813702 |  | 8000\n",
      "[10/72] OK 6844556354 |  | 4760\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[11/72] OK 13105695589 |  | 5900\n",
      "[12/72] OK 13088334040 |  | 3780\n",
      "[13/72] OK 12930766187 |  | 4190\n",
      "[14/72] OK 12765439244 |  | 1318\n",
      "[15/72] OK 12742800949 |  | 138\n",
      "[16/72] OK 12964502133 |  | 1680\n",
      "[17/72] OK 12337998446 |  | 1170\n",
      "[18/72] OK 12265535531 |  | 3790\n",
      "[19/72] OK 12199251361 |  | 1970\n",
      "[20/72] OK 12115864618 |  | 4370\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[21/72] OK 5416628605 |  | 106\n",
      "[22/72] OK 581159952 |  | 322\n",
      "[23/72] OK 581155411 |  | 218\n",
      "[24/72] OK 5288609892 |  | 1340\n",
      "[25/72] OK 639016947 |  | 203\n",
      "[26/72] OK 5314685920 |  | 394\n",
      "[27/72] OK 5414083196 |  | 172\n",
      "[28/72] OK 614018823 |  | 700\n",
      "[29/72] OK 5985326683 |  | 273\n",
      "[30/72] OK 9658901685 |  | 638\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[31/72] OK 9984435195 |  | 178\n",
      "[32/72] OK 10449719890 |  | 273\n",
      "[33/72] OK 7124720126 |  | 316\n",
      "[34/72] OK 7938675337 |  | 478\n",
      "[35/72] OK 7265378253 |  | 240\n",
      "[36/72] OK 7505587103 |  | 354\n",
      "[37/72] OK 6299869640 |  | 343\n",
      "[38/72] OK 7292744623 |  | 366\n",
      "[39/72] OK 4731396796 |  | 758\n",
      "[40/72] OK 4766358447 |  | 158\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[41/72] OK 2776148060 |  | 259\n",
      "[42/72] OK 11958276266 |  | 970\n",
      "[43/72] OK 7292711196 |  | 166\n",
      "[44/72] OK 7292730181 |  | 256\n",
      "[45/72] OK 581152025 |  | 266\n",
      "[46/72] OK 7265467400 |  | 384\n",
      "[47/72] OK 4645319961 |  | 394\n",
      "[48/72] OK 632230220 |  | 5000\n",
      "[49/72] OK 10353609341 |  | 1098\n",
      "[50/72] OK 7265532753 |  | 480\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[51/72] OK 10524561740 |  | 378\n",
      "[52/72] OK 7738766460 |  | 210\n",
      "[53/72] OK 581161141 |  | 118\n",
      "[54/72] OK 10649425078 |  | 150\n",
      "[55/72] OK 8474216403 |  | 158\n",
      "[56/72] OK 6470273781 |  | 558\n",
      "[57/72] OK 6236391549 |  | 343\n",
      "[58/72] OK 5744475486 |  | 1780\n",
      "[59/72] OK 11101926739 |  | 740\n",
      "[60/72] OK 11361008406 |  | 432\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[61/72] OK 8017386017 |  | 300\n",
      "[62/72] OK 6103130526 |  | 592\n",
      "[63/72] OK 11017719004 |  | 390\n",
      "[64/72] OK 9628077635 |  | 878\n",
      "[65/72] OK 9250656829 |  | 356\n",
      "[66/72] OK 8566701015 |  | 343\n",
      "[67/72] OK 4681232759 |  | 198\n",
      "[68/72] OK 7513817869 |  | 394\n",
      "[69/72] OK 6996108419 |  | 6360\n",
      "[70/72] OK 2977435085 |  | 178\n",
      "[Checkpoint] saved: melkin_products_master_checkpoint.csv\n",
      "[71/72] OK 8451976410 |  | 378\n",
      "[72/72] OK 7292686831 |  | 124\n",
      "[Done] saved: melkin_products_master.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "INPUT_CSV = \"melkin_products_step1_urls.csv\"\n",
    "OUTPUT_CSV = \"melkin_products_master.csv\"\n",
    "CHECKPOINT_CSV = \"melkin_products_master_checkpoint.csv\"\n",
    "\n",
    "\n",
    "def clean_int(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    nums = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(nums) if nums else 0\n",
    "\n",
    "def clean_float(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    m = re.search(r\"(\\d+(\\.\\d+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else 0.0\n",
    "\n",
    "def get_driver(headless: bool = False):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1280,2200\")\n",
    "    chrome_options.add_argument(\"--lang=ko-KR\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.implicitly_wait(6)\n",
    "    return driver\n",
    "\n",
    "def safe_get_text(driver, by, value) -> str:\n",
    "    try:\n",
    "        el = driver.find_element(by, value)\n",
    "        return el.text.strip()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_product_meta(driver):\n",
    "    \"\"\"\n",
    "    네이버 상품 상세 DOM은 변동이 잦아서\n",
    "    1) 제목(h1) 추출\n",
    "    2) 가격/평점/리뷰수는 텍스트 패턴 기반 fallback 포함\n",
    "    \"\"\"\n",
    "    # 1) 상품명: 가장 보편적으로 h1에 존재\n",
    "    name = \"\"\n",
    "    for xp in [\n",
    "        \"//h1\",\n",
    "        \"//h2\",\n",
    "        \"//*[self::h1 or self::h2][1]\",\n",
    "    ]:\n",
    "        try:\n",
    "            name = driver.find_element(By.XPATH, xp).text.strip()\n",
    "            if name and len(name) >= 2:\n",
    "                break\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "    page_text = \"\"\n",
    "    try:\n",
    "        page_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "    except Exception:\n",
    "        page_text = \"\"\n",
    "\n",
    "    # 2) 가격: \"원\" 포함 숫자 패턴\n",
    "    price = None\n",
    "    price_candidates = []\n",
    "\n",
    "    # DOM 기반 후보 (자주 바뀌므로 여러 개 시도)\n",
    "    for xp in [\n",
    "        \"//*[contains(text(), '원') and (contains(@class,'price') or contains(@class,'Price'))]\",\n",
    "        \"//*[contains(text(), '원')][1]\",\n",
    "    ]:\n",
    "        t = safe_get_text(driver, By.XPATH, xp)\n",
    "        if t:\n",
    "            price_candidates.append(t)\n",
    "\n",
    "    # 텍스트 기반 후보: 12,345원 형태\n",
    "    if page_text:\n",
    "        m_all = re.findall(r\"(\\d[\\d,]{2,})\\s*원\", page_text)\n",
    "        # 너무 많은 숫자가 나오면 상위에 등장한 값을 우선\n",
    "        if m_all:\n",
    "            # 맨 앞 5개만 후보로\n",
    "            for v in m_all[:5]:\n",
    "                price_candidates.append(v + \"원\")\n",
    "\n",
    "    # 후보에서 가장 그럴듯한(최대값이 아니라, \"할인가/판매가\" 섞이므로 최소값이 더 현실적일 때도 있음)\n",
    "    # 여기선 우선 \"가장 작은 양의 값\"을 가격으로 잡음(할인가가 흔함)\n",
    "    parsed_prices = []\n",
    "    for c in price_candidates:\n",
    "        p = clean_int(c)\n",
    "        if p > 0:\n",
    "            parsed_prices.append(p)\n",
    "    if parsed_prices:\n",
    "        price = min(parsed_prices)\n",
    "\n",
    "    # 3) 평점: 0~5 사이 소수 패턴\n",
    "    rating = None\n",
    "    # DOM 후보\n",
    "    rating_text = \"\"\n",
    "    for xp in [\n",
    "        \"//*[contains(text(),'평점')]/following::*[1]\",\n",
    "        \"//*[contains(@class,'rating') or contains(@class,'score')][1]\",\n",
    "    ]:\n",
    "        rating_text = safe_get_text(driver, By.XPATH, xp)\n",
    "        r = clean_float(rating_text)\n",
    "        if 0 < r <= 5:\n",
    "            rating = r\n",
    "            break\n",
    "    # 텍스트 기반 fallback\n",
    "    if rating is None and page_text:\n",
    "        m = re.search(r\"([0-5]\\.\\d)\\s*(?:/)?\\s*5\", page_text)\n",
    "        if m:\n",
    "            r = clean_float(m.group(1))\n",
    "            if 0 < r <= 5:\n",
    "                rating = r\n",
    "\n",
    "    # 4) 리뷰수: \"리뷰 123\" 또는 \"(123)\" 같은 패턴\n",
    "    review_count = None\n",
    "    # DOM 후보\n",
    "    for xp in [\n",
    "        \"//*[contains(text(),'리뷰') and (contains(@class,'review') or contains(@class,'Review'))][1]\",\n",
    "        \"//*[contains(text(),'리뷰')][1]\",\n",
    "    ]:\n",
    "        t = safe_get_text(driver, By.XPATH, xp)\n",
    "        rc = clean_int(t)\n",
    "        if rc > 0:\n",
    "            review_count = rc\n",
    "            break\n",
    "    # 텍스트 기반 fallback\n",
    "    if (review_count is None or review_count == 0) and page_text:\n",
    "        m = re.search(r\"리뷰\\s*([0-9,]+)\", page_text)\n",
    "        if m:\n",
    "            review_count = clean_int(m.group(1))\n",
    "\n",
    "    return {\n",
    "        \"name\": name,\n",
    "        \"price\": price,\n",
    "        \"rating\": rating,\n",
    "        \"review_count\": review_count\n",
    "    }\n",
    "\n",
    "def run_detail_scrape(headless=False, sleep_each=1.2, checkpoint_every=10):\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    # 재실행 시 체크포인트가 있으면 이어서\n",
    "    try:\n",
    "        ck = pd.read_csv(CHECKPOINT_CSV)\n",
    "        # product_id 기준으로 merge해서 이미 채운 것은 유지\n",
    "        df = df.merge(\n",
    "            ck[[\"product_id\", \"name\", \"price\", \"rating\", \"review_count\"]],\n",
    "            on=\"product_id\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_old\")\n",
    "        )\n",
    "        for col in [\"name\", \"price\", \"rating\", \"review_count\"]:\n",
    "            if f\"{col}_old\" in df.columns:\n",
    "                df[col] = df[col].fillna(df[f\"{col}_old\"])\n",
    "                df.drop(columns=[f\"{col}_old\"], inplace=True)\n",
    "        print(f\"[Resume] loaded checkpoint: {len(ck)} rows\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    driver = get_driver(headless=headless)\n",
    "    try:\n",
    "        for i, row in df.iterrows():\n",
    "            # 이미 채워진 행은 스킵\n",
    "            if pd.notna(row.get(\"name\")) and str(row.get(\"name\")).strip():\n",
    "                continue\n",
    "\n",
    "            url = row[\"url\"]\n",
    "            try:\n",
    "                driver.get(url)\n",
    "                time.sleep(2.0)  # 페이지 렌더링 대기\n",
    "                meta = extract_product_meta(driver)\n",
    "\n",
    "                df.at[i, \"name\"] = meta[\"name\"]\n",
    "                df.at[i, \"price\"] = meta[\"price\"]\n",
    "                df.at[i, \"rating\"] = meta[\"rating\"]\n",
    "                df.at[i, \"review_count\"] = meta[\"review_count\"]\n",
    "\n",
    "                print(f\"[{i+1}/{len(df)}] OK {row['product_id']} | {meta['name'][:30]} | {meta['price']}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{i+1}/{len(df)}] FAIL {row.get('product_id')} | {url} | {e}\")\n",
    "\n",
    "            time.sleep(sleep_each)\n",
    "\n",
    "            if (i + 1) % checkpoint_every == 0:\n",
    "                df.to_csv(CHECKPOINT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "                print(f\"[Checkpoint] saved: {CHECKPOINT_CSV}\")\n",
    "\n",
    "        df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        df.to_csv(CHECKPOINT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[Done] saved: {OUTPUT_CSV}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_detail_scrape(headless=False, sleep_each=1.0, checkpoint_every=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbf9c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Resume] checkpoint rows: 72\n",
      "[1/72] 12649505033 | NAVER | None | None | 0\n",
      "[2/72] 12678957567 | NAVER | None | None | 0\n",
      "[3/72] 11391657511 | NAVER | None | None | 0\n",
      "[4/72] 10108843856 | NAVER | None | None | 0\n",
      "[5/72] 11619404547 | NAVER | None | None | 0\n",
      "[6/72] 9096073807 | NAVER | None | None | 0\n",
      "[7/72] 4758250954 | NAVER | None | None | 0\n",
      "[8/72] 7124664880 | NAVER | None | None | 0\n",
      "[9/72] 10149813702 | NAVER | None | None | 0\n",
      "[10/72] 6844556354 | NAVER | None | None | 0\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'melkin_products_master_checkpoint.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32448\\1663664573.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m     \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheadless\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32448\\1663664573.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(headless, sleep_each, checkpoint_every)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mcheckpoint_every\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 180\u001b[1;33m                 \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCHECKPOINT_CSV\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8-sig\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    181\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"[Checkpoint] saved\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sontaesan\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3549\u001b[0m         )\n\u001b[0;32m   3550\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3551\u001b[1;33m         return DataFrameRenderer(formatter).to_csv(\n\u001b[0m\u001b[0;32m   3552\u001b[0m             \u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3553\u001b[0m             \u001b[0mline_terminator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mline_terminator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sontaesan\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\format.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1178\u001b[0m             \u001b[0mformatter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfmt\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         )\n\u001b[1;32m-> 1180\u001b[1;33m         \u001b[0mcsv_formatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcreated_buffer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sontaesan\\anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    239\u001b[0m         \"\"\"\n\u001b[0;32m    240\u001b[0m         \u001b[1;31m# apply compression and byte/text conversion\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         with get_handle(\n\u001b[0m\u001b[0;32m    242\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Sontaesan\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'melkin_products_master_checkpoint.csv'"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "\n",
    "\n",
    "INPUT_CSV = \"melkin_products_step1_urls.csv\"\n",
    "OUTPUT_CSV = \"melkin_products_master.csv\"\n",
    "CHECKPOINT_CSV = \"melkin_products_master_checkpoint.csv\"\n",
    "\n",
    "BRAND_TO_SMARTSTORE = True  # 핵심 스위치\n",
    "\n",
    "\n",
    "def clean_int(text: str) -> int:\n",
    "    if not text:\n",
    "        return 0\n",
    "    nums = re.sub(r\"[^0-9]\", \"\", text)\n",
    "    return int(nums) if nums else 0\n",
    "\n",
    "def clean_float(text: str) -> float:\n",
    "    if not text:\n",
    "        return 0.0\n",
    "    m = re.search(r\"(\\d+(\\.\\d+)?)\", text.replace(\",\", \"\"))\n",
    "    return float(m.group(1)) if m else 0.0\n",
    "\n",
    "def to_smartstore_url(url: str) -> str:\n",
    "    # brand.naver.com/melkin/products/123 -> smartstore.naver.com/melkin/products/123\n",
    "    if not url:\n",
    "        return url\n",
    "    u = url.split(\"?\")[0]\n",
    "    u = u.replace(\"https://brand.naver.com/\", \"https://smartstore.naver.com/\")\n",
    "    return u\n",
    "\n",
    "def get_driver(headless: bool = False):\n",
    "    chrome_options = Options()\n",
    "    if headless:\n",
    "        chrome_options.add_argument(\"--headless=new\")\n",
    "    chrome_options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    chrome_options.add_argument(\"--window-size=1280,2200\")\n",
    "    chrome_options.add_argument(\"--lang=ko-KR\")\n",
    "    chrome_options.add_argument(\n",
    "        \"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 \"\n",
    "        \"(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36\"\n",
    "    )\n",
    "    service = Service(ChromeDriverManager().install())\n",
    "    driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "    driver.implicitly_wait(10)\n",
    "    return driver\n",
    "\n",
    "def wait_body_text(driver, timeout_sec=10):\n",
    "    # 렌더링이 느릴 때 대비: body 텍스트가 일정 길이 이상 될 때까지 대기\n",
    "    t0 = time.time()\n",
    "    while time.time() - t0 < timeout_sec:\n",
    "        try:\n",
    "            txt = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "            if txt and len(txt) > 50:\n",
    "                return txt\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(0.3)\n",
    "    try:\n",
    "        return driver.find_element(By.TAG_NAME, \"body\").text\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "def extract_name(driver):\n",
    "    # smartstore 상세에서 이름은 보통 h1 또는 og:title\n",
    "    # 1) h1\n",
    "    try:\n",
    "        h1 = driver.find_element(By.XPATH, \"//h1\")\n",
    "        if h1.text.strip():\n",
    "            return h1.text.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) meta og:title\n",
    "    try:\n",
    "        og = driver.find_element(By.XPATH, \"//meta[@property='og:title']\")\n",
    "        content = og.get_attribute(\"content\") or \"\"\n",
    "        if content.strip():\n",
    "            return content.strip()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 3) title 태그\n",
    "    try:\n",
    "        title = driver.title or \"\"\n",
    "        title = title.replace(\" : 네이버 스마트스토어\", \"\").strip()\n",
    "        if title:\n",
    "            return title\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "def extract_price(driver, body_text: str):\n",
    "    # 우선 meta product:price:amount 같은 게 있으면 좋지만 없을 수 있음\n",
    "    # body 텍스트에서 \"원\" 패턴 후보를 잡아 가장 작은 값을 가격으로(할인가 기준)\n",
    "    m_all = re.findall(r\"(\\d[\\d,]{2,})\\s*원\", body_text or \"\")\n",
    "    prices = [int(v.replace(\",\", \"\")) for v in m_all[:10] if v]\n",
    "    prices = [p for p in prices if p > 0]\n",
    "    return min(prices) if prices else None\n",
    "\n",
    "def extract_rating_review(driver, body_text: str):\n",
    "    rating = None\n",
    "    review_count = None\n",
    "\n",
    "    # rating: 0~5 사이\n",
    "    m = re.search(r\"([0-5]\\.\\d)\\s*(?:/)?\\s*5\", body_text or \"\")\n",
    "    if m:\n",
    "        r = float(m.group(1))\n",
    "        if 0 < r <= 5:\n",
    "            rating = r\n",
    "\n",
    "    # review_count: \"리뷰 123\"\n",
    "    m2 = re.search(r\"리뷰\\s*([0-9,]+)\", body_text or \"\")\n",
    "    if m2:\n",
    "        review_count = int(m2.group(1).replace(\",\", \"\"))\n",
    "    else:\n",
    "        review_count = 0\n",
    "\n",
    "    return rating, review_count\n",
    "\n",
    "def run(headless=False, sleep_each=0.8, checkpoint_every=10):\n",
    "    df = pd.read_csv(INPUT_CSV)\n",
    "\n",
    "    # checkpoint resume\n",
    "    try:\n",
    "        ck = pd.read_csv(CHECKPOINT_CSV)\n",
    "        df = df.merge(\n",
    "            ck[[\"product_id\", \"name\", \"price\", \"rating\", \"review_count\"]],\n",
    "            on=\"product_id\",\n",
    "            how=\"left\",\n",
    "            suffixes=(\"\", \"_old\")\n",
    "        )\n",
    "        for col in [\"name\", \"price\", \"rating\", \"review_count\"]:\n",
    "            if f\"{col}_old\" in df.columns:\n",
    "                df[col] = df[col].fillna(df[f\"{col}_old\"])\n",
    "                df.drop(columns=[f\"{col}_old\"], inplace=True)\n",
    "        print(f\"[Resume] checkpoint rows: {len(ck)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    driver = get_driver(headless=headless)\n",
    "    try:\n",
    "        for i, row in df.iterrows():\n",
    "            # name 비어있는 것만 채우자\n",
    "            if pd.notna(row.get(\"name\")) and str(row.get(\"name\")).strip():\n",
    "                continue\n",
    "\n",
    "            url = row[\"url\"]\n",
    "            target = to_smartstore_url(url) if BRAND_TO_SMARTSTORE else url\n",
    "\n",
    "            try:\n",
    "                driver.get(target)\n",
    "                body_text = wait_body_text(driver, timeout_sec=10)\n",
    "\n",
    "                name = extract_name(driver)\n",
    "                price = extract_price(driver, body_text)\n",
    "                rating, review_count = extract_rating_review(driver, body_text)\n",
    "\n",
    "                df.at[i, \"name\"] = name\n",
    "                df.at[i, \"price\"] = price\n",
    "                df.at[i, \"rating\"] = rating\n",
    "                df.at[i, \"review_count\"] = review_count\n",
    "\n",
    "                print(f\"[{i+1}/{len(df)}] {row['product_id']} | {name[:25]} | {price} | {rating} | {review_count}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"[{i+1}/{len(df)}] FAIL {row.get('product_id')} | {target} | {e}\")\n",
    "\n",
    "            time.sleep(sleep_each)\n",
    "\n",
    "            if (i + 1) % checkpoint_every == 0:\n",
    "                df.to_csv(CHECKPOINT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "                print(\"[Checkpoint] saved\")\n",
    "\n",
    "        df.to_csv(OUTPUT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        df.to_csv(CHECKPOINT_CSV, index=False, encoding=\"utf-8-sig\")\n",
    "        print(f\"[Done] saved: {OUTPUT_CSV}\")\n",
    "\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run(headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce0e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('melkin_products_master.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce03d9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id       0\n",
       "name            72\n",
       "price            0\n",
       "rating          10\n",
       "review_count     0\n",
       "url              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601a2763",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "환경변수 NAVER_CLIENT_ID / NAVER_CLIENT_SECRET 를 설정해줘.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_32448\\2589562049.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNAVER_CLIENT_ID\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mNAVER_CLIENT_SECRET\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"환경변수 NAVER_CLIENT_ID / NAVER_CLIENT_SECRET 를 설정해줘.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[0mBASE_URL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"https://openapi.naver.com/v1/search/shop.json\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 환경변수 NAVER_CLIENT_ID / NAVER_CLIENT_SECRET 를 설정해줘."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from html import unescape\n",
    "\n",
    "NAVER_CLIENT_ID = os.getenv(\"\") # 클라이언트 아이디\n",
    "NAVER_CLIENT_SECRET = os.getenv(\"\") # 클라이언트 키\n",
    "\n",
    "if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:\n",
    "    raise RuntimeError(\"환경변수 NAVER_CLIENT_ID / NAVER_CLIENT_SECRET 를 설정해줘.\")\n",
    "\n",
    "BASE_URL = \"https://openapi.naver.com/v1/search/shop.json\"\n",
    "\n",
    "\n",
    "def strip_html(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unescape(s)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", s).strip()\n",
    "\n",
    "def to_int(x) -> int:\n",
    "    try:\n",
    "        return int(str(x).replace(\",\", \"\").strip())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def naver_shop_search(query: str, display: int = 100, start: int = 1, sort: str = \"sim\"):\n",
    "    \"\"\"\n",
    "    sort: sim(정확도), date(날짜), asc(가격오름), dsc(가격내림)\n",
    "    display: 1~100\n",
    "    start: 1~1000 (보통 API 제한/정책 확인 필요)\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": NAVER_CLIENT_ID,\n",
    "        \"X-Naver-Client-Secret\": NAVER_CLIENT_SECRET,\n",
    "    }\n",
    "    params = {\"query\": query, \"display\": display, \"start\": start, \"sort\": sort}\n",
    "    r = requests.get(BASE_URL, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def collect_for_keyword(keyword: str, pages: int = 3, display: int = 100, sort: str = \"sim\", sleep_sec: float = 0.25):\n",
    "    \"\"\"\n",
    "    keyword로 pages 만큼 페이지 수집 (page 1 = start=1, page2=start=101 ...)\n",
    "    pages*display 개까지(최대) 수집\n",
    "    \"\"\"\n",
    "    rows = []\n",
    "    for p in range(pages):\n",
    "        start = 1 + p * display\n",
    "        js = naver_shop_search(keyword, display=display, start=start, sort=sort)\n",
    "        items = js.get(\"items\", [])\n",
    "        for it in items:\n",
    "            rows.append({\n",
    "                \"collected_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"keyword\": keyword,\n",
    "                \"title\": strip_html(it.get(\"title\", \"\")),\n",
    "                \"mallName\": it.get(\"mallName\", \"\"),\n",
    "                \"brand\": it.get(\"brand\", \"\"),\n",
    "                \"maker\": it.get(\"maker\", \"\"),\n",
    "                \"category1\": it.get(\"category1\", \"\"),\n",
    "                \"category2\": it.get(\"category2\", \"\"),\n",
    "                \"category3\": it.get(\"category3\", \"\"),\n",
    "                \"category4\": it.get(\"category4\", \"\"),\n",
    "                \"lprice\": to_int(it.get(\"lprice\", 0)),\n",
    "                \"hprice\": to_int(it.get(\"hprice\", 0)),\n",
    "                \"productId\": it.get(\"productId\", \"\"),\n",
    "                \"productType\": it.get(\"productType\", \"\"),\n",
    "                \"link\": it.get(\"link\", \"\"),\n",
    "                \"image\": it.get(\"image\", \"\"),\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def add_brand_flags(df: pd.DataFrame, target_brand_keywords=(\"멜킨\", \"MELKIN\", \"melkin\")) -> pd.DataFrame:\n",
    "    def is_target(row) -> int:\n",
    "        text = \" \".join([\n",
    "            str(row.get(\"title\",\"\")),\n",
    "            str(row.get(\"brand\",\"\")),\n",
    "            str(row.get(\"mallName\",\"\")),\n",
    "            str(row.get(\"maker\",\"\")),\n",
    "        ]).lower()\n",
    "        for kw in target_brand_keywords:\n",
    "            if kw.lower() in text:\n",
    "                return 1\n",
    "        return 0\n",
    "    df = df.copy()\n",
    "    df[\"is_melkin\"] = df.apply(is_target, axis=1)\n",
    "    return df\n",
    "\n",
    "def summarize(df: pd.DataFrame):\n",
    "    # 키워드별 요약\n",
    "    grp = df.groupby(\"keyword\", dropna=False)\n",
    "    summary = grp.agg(\n",
    "        n=(\"title\", \"count\"),\n",
    "        melkin_n=(\"is_melkin\", \"sum\"),\n",
    "        melkin_share=(\"is_melkin\", \"mean\"),\n",
    "        price_mean=(\"lprice\", \"mean\"),\n",
    "        price_median=(\"lprice\", \"median\"),\n",
    "        price_p10=(\"lprice\", lambda x: x.quantile(0.10)),\n",
    "        price_p90=(\"lprice\", lambda x: x.quantile(0.90)),\n",
    "    ).reset_index()\n",
    "    summary[\"melkin_share\"] = (summary[\"melkin_share\"] * 100).round(1)\n",
    "    summary[\"price_mean\"] = summary[\"price_mean\"].round(0)\n",
    "    summary[\"price_median\"] = summary[\"price_median\"].round(0)\n",
    "    summary[\"price_p10\"] = summary[\"price_p10\"].round(0)\n",
    "    summary[\"price_p90\"] = summary[\"price_p90\"].round(0)\n",
    "    return summary\n",
    "\n",
    "def main():\n",
    "    # 멜킨 제품군/시장 키워드\n",
    "    keywords = [\n",
    "        \"덤벨\",\n",
    "        \"케틀벨\",\n",
    "        \"치닝디핑\",\n",
    "        \"철봉\",\n",
    "        \"실내자전거\",\n",
    "        \"요가매트\",\n",
    "        \"마사지건\",\n",
    "        \"홈트 기구\",\n",
    "        # 브랜드 포함 검색도 같이 (브랜드 자사 포지션 확인)\n",
    "        \"멜킨 덤벨\",\n",
    "        \"멜킨 치닝디핑\",\n",
    "    ]\n",
    "\n",
    "    # 각 키워드당 2~3페이지면 포폴 충분 (200~300개/키워드)\n",
    "    pages = 2\n",
    "    display = 100\n",
    "    sort = \"sim\"\n",
    "\n",
    "    all_df = []\n",
    "    for kw in keywords:\n",
    "        print(f\"Collecting: {kw}\")\n",
    "        df_kw = collect_for_keyword(kw, pages=pages, display=display, sort=sort)\n",
    "        all_df.append(df_kw)\n",
    "\n",
    "    df = pd.concat(all_df, ignore_index=True).drop_duplicates(subset=[\"keyword\", \"link\", \"productId\"], keep=\"first\")\n",
    "    df = add_brand_flags(df, target_brand_keywords=(\"멜킨\", \"MELKIN\", \"melkin\"))\n",
    "\n",
    "    # 저장\n",
    "    df.to_csv(\"naver_shop_raw.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved: naver_shop_raw.csv\")\n",
    "\n",
    "    summary = summarize(df)\n",
    "    summary.to_csv(\"naver_shop_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved: naver_shop_summary.csv\")\n",
    "\n",
    "    # 콘솔 미리보기\n",
    "    print(\"\\n=== Summary (keyword-level) ===\")\n",
    "    print(summary.sort_values(\"melkin_share\", ascending=False).to_string(index=False))\n",
    "\n",
    "    # 멜킨만 따로\n",
    "    df_melkin = df[df[\"is_melkin\"] == 1].copy()\n",
    "    df_melkin.to_csv(\"naver_shop_melkin_only.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "    print(\"Saved: naver_shop_melkin_only.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea0003be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "{'lastBuildDate': 'Mon, 16 Feb 2026 16:13:28 +0900', 'total': 440722, 'start': 1, 'display': 5, 'items': [{'title': '헬스럽 무게조절 조립 <b>덤벨</b> 세트 20kg', 'link': 'https://search.shopping.naver.com/catalog/52187278625', 'image': 'https://shopping-phinf.pstatic.net/main_5218727/52187278625.20241230111537.jpg', 'lprice': '32800', 'hprice': '', 'mallName': '네이버', 'productId': '52187278625', 'productType': '1', 'brand': '헬스럽', 'maker': '', 'category1': '스포츠/레저', 'category2': '헬스', 'category3': '아령', 'category4': ''}, {'title': '멜킨 롤튼 무게조절 <b>덤벨</b> 프레스 24kg 2kg 단위 중량조절 조립식 세트', 'link': 'https://smartstore.naver.com/main/products/6996108419', 'image': 'https://shopping-phinf.pstatic.net/main_8454060/84540608741.2.jpg', 'lprice': '159000', 'hprice': '', 'mallName': '멜킨스포츠', 'productId': '84540608741', 'productType': '2', 'brand': '멜킨스포츠', 'maker': '멜킨스포츠', 'category1': '스포츠/레저', 'category2': '헬스', 'category3': '아령', 'category4': ''}, {'title': '홈투더짐 클래식 무게조절 <b>덤벨</b> <b>아령</b> 세트 조립 조립식 바벨 20kg', 'link': 'https://smartstore.naver.com/main/products/4857503832', 'image': 'https://shopping-phinf.pstatic.net/main_8240202/82402027141.5.jpg', 'lprice': '30800', 'hprice': '', 'mallName': '홈투더짐', 'productId': '82402027141', 'productType': '2', 'brand': '홈투더짐', 'maker': '', 'category1': '스포츠/레저', 'category2': '헬스', 'category3': '아령', 'category4': ''}, {'title': '무게조절<b>덤벨</b> 조립 <b>아령</b> 20kg 홈짐 클럽용 바벨 MASTERPIECE MARK2', 'link': 'https://smartstore.naver.com/main/products/11183997431', 'image': 'https://shopping-phinf.pstatic.net/main_8872850/88728507753.5.jpg', 'lprice': '169000', 'hprice': '', 'mallName': '듀애슬론 스포츠', 'productId': '88728507753', 'productType': '2', 'brand': '', 'maker': '', 'category1': '스포츠/레저', 'category2': '헬스', 'category3': '아령', 'category4': ''}, {'title': '홈투더짐 프리미엄 무게조절 <b>덤벨</b> 세트 조립 <b>아령</b> 가정용 조립식 바벨 20kg', 'link': 'https://smartstore.naver.com/main/products/4885233257', 'image': 'https://shopping-phinf.pstatic.net/main_8242975/82429756745.3.jpg', 'lprice': '31800', 'hprice': '', 'mallName': '홈투더짐', 'productId': '82429756745', 'productType': '2', 'brand': '홈투더짐', 'maker': '', 'category1': '스포츠/레저', 'category2': '헬스', 'category3': '아령', 'category4': ''}]}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "NAVER_CLIENT_ID = \"ilHfeQvRIGp1ApaBoTjO\"\n",
    "NAVER_CLIENT_SECRET = \"TJyTqemEde\"\n",
    "\n",
    "url = \"https://openapi.naver.com/v1/search/shop.json\"\n",
    "headers = {\n",
    "    \"X-Naver-Client-Id\": NAVER_CLIENT_ID,\n",
    "    \"X-Naver-Client-Secret\": NAVER_CLIENT_SECRET,\n",
    "}\n",
    "\n",
    "params = {\n",
    "    \"query\": \"덤벨\",\n",
    "    \"display\": 5\n",
    "}\n",
    "\n",
    "res = requests.get(url, headers=headers, params=params)\n",
    "print(res.status_code)\n",
    "print(res.json())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb5fbc4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collect: 덤벨\n",
      "collect: 케틀벨\n",
      "collect: 바벨\n",
      "collect: 아령\n",
      "collect: 치닝디핑\n",
      "collect: 철봉\n",
      "collect: 풀업바\n",
      "collect: 실내자전거\n",
      "collect: 스핀바이크\n",
      "collect: 요가매트\n",
      "collect: 폼롤러\n",
      "collect: 마사지건\n",
      "collect: 홈트 기구\n",
      "ALL: 6472 | MELKIN: 335\n",
      "keyword                                                 title  lprice mallName brand   productId\n",
      "     덤벨             멜킨 롤튼 무게조절 덤벨 프레스 24kg 2kg 단위 중량조절 조립식 세트  159000    멜킨스포츠 멜킨스포츠 84540608741\n",
      "     덤벨 멜킨 육각덤벨 1kg 2kg 3kg 4kg 5kg 6kg 7kg 8kg 9kg 10kg 20kg    2500    멜킨스포츠 멜킨스포츠 18384787916\n",
      "     덤벨                                    멜킨스포츠 무게조절 덤벨 24kg   89000      네이버 멜킨스포츠 52203981620\n",
      "     덤벨                                       멜킨스포츠 육각 덤벨 5kg   12000      네이버 멜킨스포츠 52190642645\n",
      "     덤벨                                    멜킨스포츠 무게조절 덤벨 40kg  159000      네이버 멜킨스포츠 52204183620\n",
      "     덤벨                          멜킨스포츠 롤튼 무게조절 덤벨 24kg 단위 4kg  119000      네이버 멜킨스포츠 52340836618\n",
      "     덤벨                          멜킨스포츠 롤튼 무게조절 덤벨 36kg 단위 4kg  149000      네이버 멜킨스포츠 52378604618\n",
      "     덤벨                                       멜킨스포츠 육각 덤벨 1kg    2400      네이버 멜킨스포츠 52190646619\n",
      "     덤벨                                 멜킨스포츠 미용아령 스위티 덤벨 2kg   10500      네이버 멜킨스포츠 52252737629\n",
      "     덤벨                                      멜킨스포츠 육각 덤벨 10kg   24000      네이버 멜킨스포츠 52190640656\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from html import unescape\n",
    "\n",
    "NAVER_CLIENT_ID = \"ilHfeQvRIGp1ApaBoTjO\"\n",
    "NAVER_CLIENT_SECRET = \"TJyTqemEde\"\n",
    "\n",
    "BASE_URL = \"https://openapi.naver.com/v1/search/shop.json\"\n",
    "\n",
    "def strip_html(s: str) -> str:\n",
    "    if not s:\n",
    "        return \"\"\n",
    "    s = unescape(s)\n",
    "    return re.sub(r\"<[^>]+>\", \"\", s).strip()\n",
    "\n",
    "def to_int(x) -> int:\n",
    "    try:\n",
    "        return int(str(x).replace(\",\", \"\").strip())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def naver_shop_search(query: str, display: int = 100, start: int = 1, sort: str = \"sim\"):\n",
    "    headers = {\n",
    "        \"X-Naver-Client-Id\": NAVER_CLIENT_ID,\n",
    "        \"X-Naver-Client-Secret\": NAVER_CLIENT_SECRET,\n",
    "    }\n",
    "    params = {\"query\": query, \"display\": display, \"start\": start, \"sort\": sort}\n",
    "    r = requests.get(BASE_URL, headers=headers, params=params, timeout=15)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "\n",
    "def collect_keyword(keyword: str, pages: int = 3, display: int = 100, sort: str = \"sim\", sleep_sec: float = 0.25):\n",
    "    rows = []\n",
    "    for p in range(pages):\n",
    "        start = 1 + p * display\n",
    "        js = naver_shop_search(keyword, display=display, start=start, sort=sort)\n",
    "        for it in js.get(\"items\", []):\n",
    "            rows.append({\n",
    "                \"collected_at\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                \"keyword\": keyword,\n",
    "                \"title\": strip_html(it.get(\"title\", \"\")),\n",
    "                \"lprice\": to_int(it.get(\"lprice\", 0)),\n",
    "                \"mallName\": it.get(\"mallName\", \"\"),\n",
    "                \"brand\": it.get(\"brand\", \"\"),\n",
    "                \"maker\": it.get(\"maker\", \"\"),\n",
    "                \"category1\": it.get(\"category1\", \"\"),\n",
    "                \"category2\": it.get(\"category2\", \"\"),\n",
    "                \"category3\": it.get(\"category3\", \"\"),\n",
    "                \"category4\": it.get(\"category4\", \"\"),\n",
    "                \"productId\": it.get(\"productId\", \"\"),\n",
    "                \"productType\": it.get(\"productType\", \"\"),\n",
    "                \"link\": it.get(\"link\", \"\"),\n",
    "                \"image\": it.get(\"image\", \"\"),\n",
    "            })\n",
    "        time.sleep(sleep_sec)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def is_melkin_row(row) -> bool:\n",
    "    text = \" \".join([\n",
    "        str(row.get(\"title\",\"\")),\n",
    "        str(row.get(\"brand\",\"\")),\n",
    "        str(row.get(\"mallName\",\"\")),\n",
    "        str(row.get(\"maker\",\"\")),\n",
    "    ]).lower()\n",
    "    return (\"멜킨\" in text) or (\"melkin\" in text)\n",
    "\n",
    "def main():\n",
    "    keywords = [\n",
    "        \"덤벨\", \"케틀벨\", \"바벨\", \"아령\",\n",
    "        \"치닝디핑\", \"철봉\", \"풀업바\",\n",
    "        \"실내자전거\", \"스핀바이크\",\n",
    "        \"요가매트\", \"폼롤러\", \"마사지건\",\n",
    "        \"홈트 기구\"\n",
    "    ]\n",
    "\n",
    "    pages = 5      # 키워드당 500개(100*5) 표본\n",
    "    display = 100\n",
    "    sort = \"sim\"\n",
    "\n",
    "    dfs = []\n",
    "    for kw in keywords:\n",
    "        print(\"collect:\", kw)\n",
    "        dfs.append(collect_keyword(kw, pages=pages, display=display, sort=sort))\n",
    "\n",
    "    raw = pd.concat(dfs, ignore_index=True)\n",
    "    raw = raw.drop_duplicates(subset=[\"keyword\", \"productId\", \"link\"])\n",
    "\n",
    "    raw.to_csv(\"naver_shop_raw_all.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    melkin = raw[raw.apply(is_melkin_row, axis=1)].copy()\n",
    "    melkin.to_csv(\"naver_shop_melkin_only.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "    print(\"ALL:\", len(raw), \"| MELKIN:\", len(melkin))\n",
    "    print(melkin[[\"keyword\",\"title\",\"lprice\",\"mallName\",\"brand\",\"productId\"]].head(10).to_string(index=False))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63273c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword   n  melkin_n  melkin_share  price_mean  price_median\n",
      "     덤벨 500        53          10.6     41015.0       23000.0\n",
      "     아령 499        53          10.6     41483.0       23100.0\n",
      "  실내자전거 500        52          10.4    272402.0      213380.0\n",
      "   요가매트 500        29           5.8     42626.0       31635.0\n",
      "  스핀바이크 498        27           5.4    376010.0      269650.0\n",
      "     바벨 500        24           4.8     44398.0       29650.0\n",
      "  홈트 기구 500        23           4.6     93793.0       51300.0\n",
      "    폼롤러 500        19           3.8     21313.0       16500.0\n",
      "     철봉 481        14           2.9     64021.0       34840.0\n",
      "    풀업바 497        14           2.8     79682.0       47140.0\n",
      "    케틀벨 497        13           2.6     56556.0       33480.0\n",
      "   마사지건 500         7           1.4     79241.0       47075.0\n",
      "   치닝디핑 500         7           1.4    144574.0       90780.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_csv(\"naver_shop_raw_all.csv\")\n",
    "melkin = pd.read_csv(\"naver_shop_melkin_only.csv\")\n",
    "\n",
    "# 키워드별 멜킨 노출 점유율\n",
    "raw[\"is_melkin\"] = raw.apply(lambda r: (\"멜킨\" in (str(r[\"brand\"])+str(r[\"mallName\"])+str(r[\"maker\"])+str(r[\"title\"]))), axis=1)\n",
    "\n",
    "summary = raw.groupby(\"keyword\").agg(\n",
    "    n=(\"title\",\"count\"),\n",
    "    melkin_n=(\"is_melkin\",\"sum\"),\n",
    "    melkin_share=(\"is_melkin\",\"mean\"),\n",
    "    price_mean=(\"lprice\",\"mean\"),\n",
    "    price_median=(\"lprice\",\"median\"),\n",
    ").reset_index()\n",
    "\n",
    "summary[\"melkin_share\"] = (summary[\"melkin_share\"]*100).round(1)\n",
    "summary[\"price_mean\"] = summary[\"price_mean\"].round(0)\n",
    "summary[\"price_median\"] = summary[\"price_median\"].round(0)\n",
    "\n",
    "summary.to_csv(\"naver_shop_keyword_summary.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(summary.sort_values(\"melkin_share\", ascending=False).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f5ad46ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keyword  total  melkin_n  melkin_share(%)\n",
      "     아령    499        53            10.62\n",
      "     덤벨    500        53            10.60\n",
      "  실내자전거    500        52            10.40\n",
      "   요가매트    500        29             5.80\n",
      "  스핀바이크    498        27             5.42\n",
      "     바벨    500        24             4.80\n",
      "  홈트 기구    500        23             4.60\n",
      "    폼롤러    500        19             3.80\n",
      "     철봉    481        14             2.91\n",
      "    풀업바    497        14             2.82\n",
      "    케틀벨    497        13             2.62\n",
      "   마사지건    500         7             1.40\n",
      "   치닝디핑    500         7             1.40\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw = pd.read_csv(\"naver_shop_raw_all.csv\")\n",
    "raw[\"is_melkin\"] = raw.apply(lambda r: (\"멜킨\" in (str(r[\"brand\"])+str(r[\"mallName\"])+str(r[\"maker\"])+str(r[\"title\"]))), axis=1)\n",
    "\n",
    "summary = raw.groupby(\"keyword\").agg(\n",
    "    total=(\"title\",\"count\"),\n",
    "    melkin_n=(\"is_melkin\",\"sum\"),\n",
    ").reset_index()\n",
    "\n",
    "summary[\"melkin_share(%)\"] = (summary[\"melkin_n\"] / summary[\"total\"] * 100).round(2)\n",
    "\n",
    "print(summary.sort_values(\"melkin_share(%)\", ascending=False).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "191e98a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시장 중앙값: 41450.0\n",
      "멜킨 중앙값: 39000.0\n",
      "가격 포지션 차이: -2450.0\n"
     ]
    }
   ],
   "source": [
    "melkin = raw[raw[\"is_melkin\"] == True]\n",
    "market_price = raw[\"lprice\"].median()\n",
    "melkin_price = melkin[\"lprice\"].median()\n",
    "\n",
    "print(\"시장 중앙값:\", market_price)\n",
    "print(\"멜킨 중앙값:\", melkin_price)\n",
    "print(\"가격 포지션 차이:\", melkin_price - market_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "460ed098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category3  count\n",
      "       아령    106\n",
      "    헬스사이클     79\n",
      "    웨이트기구     75\n",
      "     요가매트     29\n",
      "     헬스소품     19\n",
      "      케틀벨     13\n",
      "     마사지건      7\n",
      "      스텝퍼      6\n",
      "     로잉머신      1\n"
     ]
    }
   ],
   "source": [
    "melkin_cat = melkin.groupby(\"category3\").size().reset_index(name=\"count\")\n",
    "melkin_cat = melkin_cat.sort_values(\"count\", ascending=False)\n",
    "print(melkin_cat.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "63863501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "시장 중앙값: 41450.0\n",
      "멜킨 중앙값: 39000.0\n",
      "차이: -2450.0\n"
     ]
    }
   ],
   "source": [
    "market_median = raw[\"lprice\"].median()\n",
    "melkin_median = raw[raw[\"is_melkin\"] == True][\"lprice\"].median()\n",
    "\n",
    "print(\"시장 중앙값:\", market_median)\n",
    "print(\"멜킨 중앙값:\", melkin_median)\n",
    "print(\"차이:\", melkin_median - market_median)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
